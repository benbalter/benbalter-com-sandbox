{"author":"Benjamin J. Balter","title":"Publishing Government Data That Developers Will Actually Use\n","excerpt":"Despite increasing public support (as well as a number of executive mandates) publishing public data in a machine-readable format is not as simple as pressing the \"publish\" button. Why? Equally important as exposing the information itself is fostering a vibrant developer ecosystem around it. By making the publishing agency, not the public, responsible for making information immediately useful, government can lower the barriers associated with consuming its data and introduce additional citizen services at little to no cost to the agency.\n","layout":"post","categories":["Technology"],"tags":[".govs","analytics","api","apis","data","dogfooding","enterprise","federal","geospacial","gov 2.0","government","IT","json","Oauth","open data","open government","REST","RESTful","web services","xml"],"post_format":[],"url":"/2012/06/02/publishing-government-data-that-developers-will-actually-use/","date":"2012-06-02 00:00:00 -0400","id":"/2012/06/02/publishing-government-data-that-developers-will-actually-use","next":"[![Unsatisfied with your Contractor?](http://ben.balter.com/wp-content/uploads/2012/06/mike-holmes-203x300.jpeg){.alignright}][1]\n\nThere are two kinds of software: cludgy software and open source. Think about it logically. When you (or your organization) is the only person that's ever going to see something, you're a lot more likely to \"just make it work.\" After all, who would ever know? [^1]\n\nBut the same logic that applies to sweeping literal dirt under the rug doesn't apply to writing code. Whereas a rug will always serve to cover the floor, applications evolve over time and code is often constantly reused and repurposed as customers' needs change. Simply put, it's impossible to predict today where your code is going to be a year from now and it's in your best interest to plan accordingly.\n\nOpen source hedges this risk by distinguishing generic logic (say posting content online) from application-specific customization (say the use-case-specific presentation of that content). Yet when you're writing with the intention of producing proprietary or one-off code, you do everything in one pass. The true challenge arises when the same problem emerges again in another department, another business unit, or more generally in an even slightly different context. You're reinventing the wheel. You're \"open sourcing\" (even if within your organization). The solution? Always assume your software is going to be open source, even if you know it's never going to be, and here's why:\n\n**Flexible from the start** - Imagine you building a house and the contractor literally nails down all your furniture at the onset, saying you could always remove it before you sell. You'd almost certainly hire a new contractor. Even if you're never going to sell the house, you may want to get a new couch, or at the very least change a room's layout somewhere down the line. Yet software developers do it all the time. We custom build solutions, and then go back and abstract logic to \"open source\" it as needed. You're doubling the effort. Keep logic separate from implementation-specific customization, and you'll have a shared, portable solution from day one. Put another way, your business unit is no way special or unique. The same logic that presents updates about the latest line of widgets to your customers can also be used to update the same customer base about cogs and you should prepare for that potential synergy from day one, even if not immediately realized.\n\n**Modular by design **- Distinguishing unrelated components encourages several coding best practices. In addition to introducing a modular design, meaning additional components could easily be added (or existing components removed) down the line, abstraction often yields objectively more stable and more readably maintainable code due to the abhorrence of the copy-and-paste effect. Put another way, you're forced to build elegant solutions — the fact that others are not only going to see, but have to be able to use and adapt your code forces you to follow best-practices like name spacing, abstraction, and object oriented programming.\n\n**A message to your future self** – Ever go back and look at old code, [only to scratch your head][3] as to what's going on? The same you that may be asking yourself what you were thinking when you got a tattoo five years back, is also going to be asking why you wrote that singleton function five years ago. Yet when you write open source, you mitigate that risk by explaining your code in such a way that others (including your future self) can understand it. In a world of system orientated architectures and ever-changing requirements, the chance that a software project is one-and-done is increasingly rare, not to mention the fact that by failing to properly document, you're introducing a significant risk of vendor lock in. Your successor will thank you, and so will the person paying the bills.\n\nThe reality of today's business environment is that all software is inherently \"open source\", even if the scope of the sharing is limited to an organization. Assume the software is open, needs to be modular, and will be repurposed, and you will save significant costs in the long run. And when you require the same of outside contractors, you get better, more flexible code, and offset the risks of vendor or technology lock in in the long run.\n\nJustice Brandeis is famous for noting that \"sunlight is the best disinfectant.\" Likewise, the transparency afforded by the open-source ethos produces [more reliable software][4] –  so why not simply assume your code is going to be open source from the start?\n\n[^1]: The same would apply when you're buying software and the contractor is under the impression no one outside the organization will ever see the code, and more importantly, the code could never negatively impact the public's perception of their overall work-product [5]\n\n [1]: http://ben.balter.com/wp-content/uploads/2012/06/mike-holmes.jpeg\n [2]: \"The same would apply when you're buying software and the contractor is under the impression no one outside the organization will ever see the code, and more importantly, the code could never negatively impact the public's perception of their overall work-product\"\n [3]: https://twitter.com/BenBalter/status/209356982983999488\n [4]: http://www.coverity.com/library/pdf/coverity-scan-2011-open-source-integrity-report.pdf","previous":"<blockquote>\n<p>&#8220;Sharepoint is a plague upon the American workforce. This ubiquitous piece of collaboration software has taught millions of people that Intranets are destined to be places where you can&#8217;t find anything<span>, but it</span> doesn&#8217;t have to be this way, despite what Microsoft may have you believe.&#8221;</p>\n</blockquote><p>— Joe Flood on <a href='http://joeflood.com/2012/05/10/free-yourself-from-the-tyranny-of-sharepoint/'>Freeing Yourself from the Tyranny of Sharepoint</a></p>","content":"<p><a href='http://www.governmentciomagazine.com'><img alt='' class='alignright' src='http://ben.balter.com/wp-content/uploads/2012/06/government-cio-magazine-june-2012.png' /></a>Despite increasing public support (as well as a number of executive mandates) publishing public data in a machine-readable format is not as simple as pressing the &#8220;publish&#8221; button. Why? Equally important as exposing the information itself is fostering a vibrant developer ecosystem around it. By making the publishing agency, not the public, responsible for making information immediately useful, government can lower the barriers associated with consuming its data and introduce additional citizen services at little to no cost to the agency.</p><p><strong>1. Garbage in, garbage out.</strong> Good, clean data may be surprisingly difficult to come by, especially when working with government systems that have been coupled together over decades. Data standards and conventions change, mechanisms of data collection evolve, and the data itself may be interpreted differently as new policies are introduced. As a result consistent practices, like naming conventions or data formats, often go overlooked. Where practical, take steps to normalize the data prior to release, rather than pushing the responsibility off to be inefficiently repeated by each application individually.</p><p><strong>2. Eat your own dog food.</strong> When organizations consume the products they create, they empirically deliver better, more reliable, and more innovative products. You&#8217;d never seek to buy a car from a dealer that&#8217;s never driven one, yet we often expect the public to build applications based on APIs (Application Programming Interfaces – how computers talk to one another) published by organizations that have never had to consume their own data. Rather than solving the same problem twice, start by exposing all relevant data through public APIs and then work backward to build internal applications that rely on those externally facing data feeds.</p><p><strong>3. Data as a citizen service.</strong>It is tempting to try and meet open data benchmarks, at least on face, by publishing snapshots of large datasets. Yet multi-gigabyte database exports do little to encourage external development, especially when such data-dumps are delayed and infrequent. Imagine the usefulness of a Facebook feed that showed your friends&#8217; activity from last month. Datasets should be directly exposed so that the public has access to live, real-time data, either in its entirety, or through proper access controls. This not only allows agencies to deliver more useful information, but also reduces the need to store the same data in multiple formats and in multiple locations.</p><p><strong>4. Curate discrete pieces of data.</strong>APIs are most useful when they do the heavy lifting for those consuming them, especially in terms of sifting through large amounts of data. In practical terms that means returning data to the most discrete level possible, be it a single row, rather than merely returning a subset of the dataset, or even returning a single cell. Seemingly obvious but often overlooked, a query for the broadband speeds at a given address, for example, should return only the data relating to that address, not the entire city or even state-wide dataset. By allowing developers to query the data directly that means they will need less development time on their end, and thus a higher likelihood that an application will be built.</p><p><strong>5. Serve data in multiple formats.</strong>When providing a service, whether you are a waiter or a CIO, &#8220;the customer is always right.&#8221; In the context of APIs, that means you need to return the information in the developers&#8217; native tongue, not the server&#8217;s. For some languages, heavyweight methods like XML may make sense, for others, especially mobile applications, JSON or JSONP may be preferred. Be prepared to return data in multiple formats, even as those formats continue to evolve.</p><p><strong>6. Minimize the handshake learning curve</strong>. Authentication may often be necessary, but the pain associated with it does not have to. Ensure that developers can easily register API keys, with minimal effort and without delay, and rely on common authentication frameworks (e.g., OAuth 2.0) to minimize the learning curve. Similarly, whenever practical, provide common API wrappers and other software development kits in multiple languages.</p><p><strong>7. Encourage adoption through documentation.</strong> Often, the most overlooked aspect of exposing data is documentation. Describe the structure of the data fully, including how to interpret it, and ensure that any technical documentation such as lists of methods and sample code is both complete and accurate. The only thing worse than not having documentation is having wrong documentation. The best APIs even provide Wikis to allow developers to share tools and best practices with one another.</p><p><strong>8. Follow industry standards and convention</strong>. Although APIs may just be beginning to take foothold in government, a set of best practices have quietly evolved in the private sector over the past several years. What may seem like small, technicalities, such as a truly RESTful API or using proper HTTP methods like GET, POST, PUT, and DELETE, for example, can mean the difference between useful and useless.</p><p><strong>9. Bake in Analytics</strong>. When it comes to garnering support for future efforts, nothing can be more powerful than raw numbers. From the ground up, bake in analytics on both the application level (what applications are querying the API?), and across APIs on the dataset levels (what APIs are being used?). This will help establish data-driven priorities, such as what type of data may be a good candidate for future APIs.</p><p><strong>10. Location, location, location.</strong>With the &#8220;consumerization&#8221; of mobile, datasets are increasingly becoming location based. It&#8217;s not what datasets are out there, but rather, what datasets are out there about my immediate world. Likewise, government datasets are increasingly about where, just as much as it is about what. It&#8217;s important that this reality be taken into account when building APIs by incorporating geospatial lookups within the API, such a relying on MongoDB or other location-aware data structures.</p><p>Exposing data as a service is quickly becoming an industry-standard practice. Many popular startups owe their success to the vibrant app communities that surround them, grown simply by lifting some of the burden off of developers&#8217; shoulders. Government agencies may not yet be able to publish data with the mere click of a button, but when done right from the start, exposing additional data sources may be a more trivial task than many expect, and will in turn deliver value to citizens in ways today unimagined by agencies.</p><p><em>This is an excerpt of an article originally published in the June 2012 issue of <a href='http://www.governmentciomagazine.com'>Government CIO Magazine</a>.</em> <strong><a href='http://www.governmentciomagazine.com/2012/06/ten-steps-publishing-government-data-developers-will-actually-use#blog-content'>Continue Reading →</a></strong></p>","related_posts":[{"url":"/2011/11/29/towards-a-more-agile-government/","title":"Towards a More Agile Government"},{"url":"/2011/09/01/why-wordpress/","title":"Why WordPress"},{"url":"/2012/03/05/wordpress-for-government-and-enterprise/","title":"WordPress for Government - A Problem of Perception"},{"url":"/2011/11/01/federal-agility-a-cultural-solution-to-a-technical-problem/","title":"Federal Agility: a Cultural Solution to a Technical Problem"},{"url":"/2010/11/15/will-federal-contracting-officers-soon-have-their-heads-in-the-clouds/","title":"Will Federal Contracting Officers Soon Have Their Heads in the Clouds?\n"},{"url":"/2012/04/10/whats-missing-from-cfpbs-awesome-new-source-code-policy/","title":"What's Missing from CFPB's Awesome New Source Code Policy"},{"url":"/2012/12/15/why-wordpress-next-version-should-just-give-it-a-rest-already/","title":"Why WordPress's next version should just give it a REST already"},{"url":"/2011/08/29/wp-document-revisions-document-management-version-control-wordpress/","title":"WP Document Revisions — Document Management & Version Control for WordPress"},{"url":"/2011/08/31/enterprise-open-source-and-why-better-is-not-enough/","title":"Enterprise, Open Source, and Why Better is not Enough\n"},{"url":"/2011/01/04/the-files-in-the-computer/","title":"The Files \"in\" the Computer -- Zoolander and the California Supreme Court"}]}