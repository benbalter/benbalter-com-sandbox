{"author":"Benjamin J. Balter","title":"Government's Release of Federally Funded Source Code: Public Domain or Open Source? Yes.","excerpt":"The question for developers isn't how should the US government best license software, but how can the open source community help it to do so","layout":"post","categories":["Law","Technology"],"tags":[".govs","code","contracting","copyright","enterprise","federal","gov 2.0","government","gpl","IT","licensing","open government","open source","procurement"],"post_format":[],"url":"/2012/07/26/government-release-of-source-code-public-domain-or-open-source/","date":"2012-07-26 00:00:00 -0400","id":"/2012/07/26/government-release-of-source-code-public-domain-or-open-source","next":"You may notice things are bit snappier around here these days, having [recently converted](https://github.com/benbalter/wordpress-to-jekyll-exporter) the site from WordPress, to [Jekyll](https://github.com/mojombo/jekyll).[^4]\n\nJekyll is a blog-aware static site generator — heavily integrated with the social code sharing service GitHub — the move to which, was primarily motivated by a desire to embrace the brave new, [post-CMS world](http://developmentseed.org/blog/2012/07/27/build-cms-free-websites/) we now find ourselves in. While WordPress is great, [130 outages over the past six months (totalling more than a day's worth of downtime)](http://cl.ly/image/1M420a152e1z), left a bit to be desired in terms of hosting. \n\nAlthough powered by the open-source CMS WordPress, the old site (shared hosting provided by Bluehost) for performance sake, would actually just served flat HTML and Javscript files from disk (generated on a regular basis by an industry-standard plugin known as  [W3 Total Cache](http://wordpress.org/extend/plugins/w3-total-cache/)), but fired up WordPress on every request (on top of the already slugish Apache). \n\nDon't get me wrong. WordPress can be [configured to fly](http://wordpress.org/extend/plugins/batcache/) given the right setup, and that's exactly what I set out to do. I got the best of the best. I spun up a shiny new AWS box, got Nginx with microcache up and running, APC for opcode, page, and object cache, and even put everything behind Varnish.\n\nBut as much as it pains the developer in me, just like fixies, PBR, and Javascript, static sites are back in style. Reduce the complexity, push it to the edge, and let the visitor's browser call APIs directly to generate any dynamic content you may need. Same functionality, same experience, no headache.\n\nThe pitch is straightforward. It leads to simple, flexible, and reliable websites that allow for a renewed focus on what actually matters: the content. Dave Cole over at [Development Seed](http://developmentseed.org/) (also powered by Jekyll) [put it best](http://developmentseed.org/blog/2012/07/27/build-cms-free-websites/):\n\n> In the past, building websites with features like consistent templates and lists of aggregated content meant setting up complex content management systems. These CMSs consisted of templating logic, application code, and content databases so they could assemble webpages each time they were requested by site visitors. They were complicated systems that depend on many separate applications working together, like a web server to route page requests to a PHP application that uses pre-defined page layout templates to format content that’s stored in a MySQL database. Serving a page request required at least three separate applications all working together — any one failing would bring down the system...\n\n> From open source frameworks like Drupal, Wordpress, and Expression Engine to multi-million dollar proprietary applications that the government and big corporations procure from companies that also build tanks and battle ships, these systems produce the same exact output: HTML, CSS, and JavaScript files that web browsers know how to format into the webpages we see. Additional features like RSS or JSON API feeds are just new templates for the same content, and backend workflow modules like those for embedded media and handling email notifications are really separate systems that introduce complexity when integrated with the publishing system.\n\nAnd then there's cost. Putting aside the value of time for a moment, shared hosting's going to run you in the ballpark of $7 a month; AWS starts at $14, plus the cost of bandwidth and storage; and Jekyll, if hosted by GitHub? Free.[^3]\n\nI stood up the three options side-by-side, and ran them through the riggors of a performance testing tool humerously called [Siege](http://www.joedog.org/siege-home/), the results of which can be found below.\n\nI'm still unpacking some of the boxes of bytes, so if you notice something that doesn't seem right, feel free to [open an issue](https://github.com/benbalter/benbalter.github.com/issues), or better yet, like what you see, feel free to [fork and contribute](https://github.com/benbalter/benbalter.github.com). Embracing somewhat of a crawl, walk, run, or fail-fast philosohpy, next up is [outputting the pages as JSON](https://github.com/benbalter/benbalter.github.com/blob/js/_plugins/generate-json.rb) and relying on Backbone to do the heavy lifting.\n\nIs it the [first shots](http://presidential-innovation-fellows.github.com/mygov/) [of a static-site](http://presidential-innovation-fellows.github.com/rfpez-blog/) [revolution](http://presidential-innovation-fellows.github.com/bluebutton/)? Time will tell. \n\nThe CMS is dead. Long live the CMS.\n\n***\n\n## The Results\n\n**WARNING: Geek Content!**\n\n### Homepage\n\nCommand: `siege -c 20 -t 30S -b ben.balter.com`\n\nThe first test was to benchmark the homepage, the most heavily trafficed page on the site. Given 30 seconds of continuous traffic from 20 concurrent users, Bluehost was able to serve a meager 40 users. AWS managed an impressive 2000 users during that same time period (a 50x performance improvement), and did so twice as fast. Enter Jekyll with more than 2600 users (65x increase), responding on average to each in less than a quarter of a second.\n\n#### Shared Hosting (Bluehost)\n\n\tTransactions:\t\t          40 hits\n\tAvailability:\t\t      100.00 %\n\tElapsed time:\t\t       29.54 secs\n\tData transferred:\t        0.68 MB\n\tResponse time:\t\t        0.57 secs\n\tTransaction rate:\t        1.35 trans/sec\n\tThroughput:\t\t        0.02 MB/sec\n\tConcurrency:\t\t        0.78\n\tSuccessful transactions:          40\n\tFailed transactions:\t           0\n\tLongest transaction:\t        0.71\n\tShortest transaction:\t        0.47\n\n#### Varnish + Microcache + Page Cache + Object Cache (AWS)\n\n\tTransactions:\t\t        1954 hits\n\tAvailability:\t\t      100.00 %\n\tElapsed time:\t\t       29.39 secs\n\tData transferred:\t       13.63 MB\n\tResponse time:\t\t        0.30 secs\n\tTransaction rate:\t       66.49 trans/sec\n\tThroughput:\t\t        0.46 MB/sec\n\tConcurrency:\t\t       19.80\n\tSuccessful transactions:        1954\n\tFailed transactions:\t           0\n\tLongest transaction:\t        0.92\n\tShortest transaction:\t        0.06\n\n#### Github Pages\n\n\tTransactions:\t\t        2629 hits\n\tAvailability:\t\t      100.00 %\n\tElapsed time:\t\t       29.42 secs\n\tData transferred:\t        2.71 MB\n\tResponse time:\t\t        0.22 secs\n\tTransaction rate:\t       89.36 trans/sec\n\tThroughput:\t\t        0.09 MB/sec\n\tConcurrency:\t\t       19.86\n\tSuccessful transactions:        2629\n\tFailed transactions:\t           0\n\tLongest transaction:\t        1.38\n\tShortest transaction:\t        0.06\n\t\n### 404s\n\nCommand: `siege -c 20 -t 30S -b ben.balter.com/aaaaaaa/`\n\nThe true challenge comes in not from serving a static front page (which is presumably cached by WordPress after the first request), but in what happens when it has to reach into the database to retrieve content, for example, when processing a page that doesn't exist.[^1] Bluehost squeezed out a single response each second, AWS just over 50, and Jekyll didn't flinch at 80.\n\n#### Shared Hosting (Bluehost)\n\n\tTransactions:\t\t          30 hits\n\tAvailability:\t\t       21.43 %\n\tElapsed time:\t\t       29.58 secs\n\tData transferred:\t        0.19 MB\n\tResponse time:\t\t       14.93 secs\n\tTransaction rate:\t        1.01 trans/sec\n\tThroughput:\t\t        0.01 MB/sec\n\tConcurrency:\t\t       15.14\n\tSuccessful transactions:           0\n\tFailed transactions:\t         110\n\tLongest transaction:\t       22.88\n\tShortest transaction:\t        0.00\n\n#### Varnish + Microcache + Page Cache + Object Cache (AWS)\n\n\tTransactions:\t\t        1567 hits\n\tAvailability:\t\t      100.00 %\n\tElapsed time:\t\t       29.13 secs\n\tData transferred:\t       14.71 MB\n\tResponse time:\t\t        0.37 secs\n\tTransaction rate:\t       53.79 trans/sec\n\tThroughput:\t\t        0.50 MB/sec\n\tConcurrency:\t\t       19.83\n\tSuccessful transactions:           0\n\tFailed transactions:\t           0\n\tLongest transaction:\t        1.13\n\tShortest transaction:\t        0.00\n\n#### Github Pages\n\n\tTransactions:\t\t        2373 hits\n\tAvailability:\t\t      100.00 %\n\tElapsed time:\t\t       29.82 secs\n\tData transferred:\t       10.48 MB\n\tResponse time:\t\t        0.25 secs\n\tTransaction rate:\t       79.58 trans/sec\n\tThroughput:\t\t        0.35 MB/sec\n\tConcurrency:\t\t       19.92\n\tSuccessful transactions:           0\n\tFailed transactions:\t           0\n\tLongest transaction:\t        1.42\n\tShortest transaction:\t        0.00\n    \n### Uptime\n\nThe other concern was uptime. With the AWS route you may get the performance, but with all that complexity, it's increasingly more like that something would go wrong, harder to diagnose and resolve, and unlike shared or managed hosting, if your site goes down at 3:00 am, the only person to call is yourself. (no thanks.)\n\nWith Jekyll, because the files are simply sitting on the server, absent a catastrophic failure, when things go wrong with Jekyll, it simply keeps serving the old content.[^2]\n    \n## Conclusion \n\nIt's cheaper, it's faster, it's simpler, it's worry free, and in my opinion, it's the future. Welcome to the post-CMS world.\n\n[^1]: Requesting a page that doesn't exist will require WordPress to run multiple database queries to attempt to find the page, a request that would most likely not be cached in the event that the 404 was sent in error.\n\n[^2]: GitHub's build queue has been backing up every once in a while as of late, but if a change isn't instantanous, I'm okay with that.\n\n[^3]: That's free as in speech **and** free as in beer.\n\n[^4]: Not to be confused with [The Jackal](http://www.youtube.com/watch?v=Q7H_L5cYkg8).","previous":"<p><a href='http://ben.balter.com/wp-content/uploads/2012/07/dashboard-all-the-things.jpeg'><img alt='Dashboard all the things' class='alignright' src='http://ben.balter.com/wp-content/uploads/2012/07/dashboard-all-the-things-300x225.jpeg' /></a></p><p>I was recently asked how I would architect a personalized dashboard experience for visitors to a large, customer-facing website. My response? <em>I wouldn&#8217;t.</em></p><p>A dashboard in a car or airplane makes sense. It&#8217;s not as if I could click &#8220;speedometer&#8221; while driving or press the &#8220;altimeter&#8221; button while flying. I simply need everything at all times. But virtual interfaces don&#8217;t have that same limitation. In fact they don&#8217;t have any limitations. A dashboard can have as much information as the most ambitious engineer can dream — and that&#8217;s exactly the problem.</p><p>Put it in context: Google <a href='http://googleblog.blogspot.com/2012/07/spring-cleaning-in-summer.html'>recently announced the retirement of iGoogle</a>, it&#8217;s own personalized dashboard, and I second their nomination to induct dashboards into the #doingitwrong hall of fame, joining the likes of internet portals, splash pages, and well, basically anything involving ActiveX or Flash.</p><p>Dashboard were a fun user interface experiment. They really were, especially compared to the static pages they evolved from. That was the whole point of Web 2.0, wasn&#8217;t it? Personalization? I mean, it was really cool to drag and drop widgets, and build a virtual command center to monitor my little corner of the internet, and that was fine when there wasn&#8217;t much internet out there to monitor. But the web collectively hit a tipping point a few years back. From push notifications to always-on e-mail, in more ways than we imagine, we now bombard ourselves with more information that we can physically process at any given moment. <a href='http://www.apple.com/iphone/features/retina-display.html'>Quite literally</a>.</p><p>Think about it this way: when customers come to a website, they&#8217;re not looking to solve 10 problems. They&#8217;re looking to solve one. They don&#8217;t want all the potentially relevant information thrown at them all at once; they just want what they need. And they want computers to make that determination for them. But hey, this isn&#8217;t the first time those who predict our user experience needs have erred on the side of <a href='http://www.pocket-lint.com/images/dynamic/NEWS-32125-b3a8b509bc5e3a074f7f240f57d71aa9.jpg'>moar is better</a>.</p><p>So that&#8217;s it? That&#8217;s the end of simultaneous streams? <a href='http://www.informationweek.com/news/software/productivity_apps/240003296'>Far from it</a>. This once-disruptive technology now has a long journey down the Technology S Curve as it becomes the go-to solution for all the business intelligence and project analyst types that stumble across it, in other words, the late adopters.</p><p>Don&#8217;t get me wrong. I&#8217;m sure guilty of building <a href='http://my.fcc.gov/'>a dashboard</a> or <a href='http://codex.wordpress.org/Dashboard_Screen'>two</a> in my day. I&#8217;m not saying that they&#8217;ve never had a place. What I&#8217;m saying is that today, not even the most complex dashboard could give you an accurate snapshot of its genus&#8217;s future. If not dashes, then what? Beyond turning everything into a ubiquitous search box (<em>a la</em> <a href='http://gov.uk'>gov.uk</a>), I&#8217;m far from for a UI/UX expert, but I tend to think that startups generally have a pretty good sense of what&#8217;s next. They have to. If they don&#8217;t get it right the first time around, they tend not to have a second try. So what do we see?</p><ul>\n<li>\n<p><strong>Activity -</strong> Social apps like Facebook, Twitter, Foursquare, even GitHub are all built around the concept of activity. Whether its a news feed, recent checkins, or even commit activity, the question I come with is &#8220;what&#8217;s going on?&#8221; and it gets answered as in depth as I care to scroll through, not as in depth as an engineer arbitrarily decided I needed a few years back. It&#8217;s linear. It&#8217;s <a href='http://en.wikipedia.org/wiki/Inverted_pyramid'>inverted pyramid</a>. It&#8217;s customized by whom or what I follow, not by what I add or (re)arrange.</p>\n</li>\n\n<li>\n<p><strong>Minimal</strong> – Productivity apps like Gmail, Google Reader, even dropbox don&#8217;t summarize for me how many e-mails, unread posts, or free MB I have as soon as I log in, and with the exception of a few labs features here or there, don&#8217;t even give me the option to have anything more than a bare-bones inbox, unread feed, or directory listing. In fact, GMail and Google Reader were recently criticized for <a href='http://jonoscript.wordpress.com/2012/04/26/gmail-designer-arrogance-and-the-cult-of-minimalism/'>going a bit too far</a> in this direction. But the lesson is the same: just give me my stuff and get out of the way.</p>\n</li>\n\n<li>\n<p><strong>Immediate</strong> - Transactional apps, like Uber or Square focus on action, not the past (or even the present). When I open the Uber or square apps, I&#8217;m immediately presented with the ability to request a vehicle or swipe a card, not my top tasks, not an arbitrary array of options or metrics, not with recent news about the product or popular add-ons. The app simply stands at attention, awaiting orders. I actually had to dig a bit to find my transaction history and related business analytics, and I&#8217;d argue that&#8217;s a really good thing.</p>\n</li>\n</ul><p>Think about the last time you&#8217;ve used a drag-and-drop dashboard: If you&#8217;re like me, it&#8217;s going to be either Google Analytics or WordPress, and if that&#8217;s the case, it&#8217;s simply known as <em>the screen you see after you log in, but before you can do what you need to do</em>. It&#8217;s wasted pixels. It&#8217;s cruft from a bygone era when clicks were expensive and developers were left wondering &#8220;how can we fit more on a page&#8221;.</p><p>Options are a crutch. It&#8217;s the natural tendency of any engineer to over engineer a system, and that tendency is even stronger in a risk-averse, top-down culture <a href='http://www.google.com/?q=dashboard+site:.gov'>like government</a>. But your job — as an engineer, as a product manager, as user — is to push back, to fight that urge, to make <a href='http://wordpress.org/about/philosophy/'>decisions, not options</a>. Not convinced? That feature you can&#8217;t <a href='https://github.com/blog/1091-spring-cleaning'>bring yourself to cut</a>? Expose it through your API and see how many users complain.</p><p>It&#8217;s no longer a question of &#8220;is this possible technologically?&#8221;. It&#8217;s no longer a matter of &#8220;can you expose me to that information 24/7?&#8221;. Ever since the advent of <a href='http://html5zombo.com/'>Zombo com</a>, the only limit is our imagination. We&#8217;ve figured out the hard stuff. It&#8217;s not centralization and personalization. It&#8217;s decentralization and interoperability. Simplicity is the new black.</p>","content":"<p>A petition was recently posted on <a href='https://petitions.whitehouse.gov/'>We The People</a> demanding <a href='https://petitions.whitehouse.gov/petition/maximize-public-benefit-federal-technology-sharing-government-developed-software-under-open-source/6n5ZBBwf?utm_source=wh.gov&amp;utm_medium=shorturl&amp;utm_campaign=shorturl'>that federally funded software be released under an open source license</a>. Makes sense. The public should have access to what is technically their property.</p><p>However, <a href='http://www.techdirt.com/articles/20120723/12181319800/should-software-created-federal-govt-be-open-source-licensed-public-domain.shtml'>TechDirt posed the question</a> of whether it should be released under an open-source license or public domain, and I&#8217;m afraid they really missed the point.</p><p>There&#8217;s no doubt in my mind that the creator of the petition was simply asking the question &#8220;I can haz source code?&#8221; Plain and simple. Put it in context: 99% of the time when an organization (or an individual) releases software to the public, they do so under the terms of an open source license. It tells users what they can and can&#8217;t do, and tells contributors under what terms they can contribute. It&#8217;s set&#8217;s the ground rules. It&#8217;s a contract with the public. It&#8217;s a prenup for code.</p><p>So what&#8217;s the issue? Although I generally dread the phrase, in this case, government is objectively different. Under 17 U.S.C § 105 US Government Works are not subject to domestic copyright protection. It&#8217;s not technically public domain, but it&#8217;s close enough. <sup id='fnref:1'><a href='#fn:1' rel='footnote'>1</a></sup> Any US citizen can use the code any way they wish. There&#8217;s simply no copyright, thus no need to license. <sup id='fnref:2'><a href='#fn:2' rel='footnote'>2</a></sup> And this entire debate is a moot point if the software is a derivative work of a viral license like the GPL, the most common open source license. <sup id='fnref:3'><a href='#fn:3' rel='footnote'>3</a></sup></p><p>That of course, only applies to code created by a US government employee, an increasingly rare occurrence. <sup id='fnref:4'><a href='#fn:4' rel='footnote'>4</a></sup> Absent permission from the contracting officer, the US government retains unlimited rights for all work created under contract (including the right to redistribute). <sup id='fnref:5'><a href='#fn:5' rel='footnote'>5</a></sup> And again a moot point if GPL derivative (and thus must be given to the Government under the GPL.)</p><p>Yet all this is very academic (not to mention dry). Waldo Jaquith and Anil Dash <a href='https://twitter.com/anildash/statuses/227476701599391744'>made a great suggestion</a>: let&#8217;s be pragmatic here. Government doesn&#8217;t hold on to software because they are concerned about licensing. They hold on to software because they have better things to do, because it&#8217;s not within the culture, and because there&#8217;s no angry mob slamming a battering ram against the metaphorical front gates when they don&#8217;t.</p><p>I don&#8217;t think the nuances of federal procurement law is even close to the first thing we should care about here. <sup id='fnref:6'><a href='#fn:6' rel='footnote'>6</a></sup> The concern is about whether feds should do the leg work to open source it or not. The question for us as developers, for the thought leaders in the space, isn&#8217;t how should the US government best license / not license software, but <em>how can the open source community help it to do so.</em> How can we get more software out the door? In a world of finite time, <em>how can we make open sourcing</em> <sup id='fnref:7'><a href='#fn:7' rel='footnote'>7</a></sup> <em>a bonafide priority</em>?</p><p>How? For one, involvement in existing open source projects <sup id='fnref:8'><a href='#fn:8' rel='footnote'>8</a></sup> would surely send a strong message that there&#8217;s latent demand here, and would give the foot soldiers political air cover to forge onward with their efforts. For another, taking ownership of the code itself, and realizing it is <em>our</em> code, not the government&#8217;s would surely change the tone of the debate by encouraging agencies to ship code sooner, rather than delaying release out of fear of criticism.</p><p>Put simply, it&#8217;s about what role we are going to play, not what rights we are going to receive. Let&#8217;s at least get the source code, then we can go back to our regularly scheduled holy wars over licensing.</p><p><em>As always, <a href='http://ben.balter.com/fine-print/'>views are my own</a>.</em></p><div class='footnotes'><hr /><ol><li id='fn:1'>\n<p>I&#8217;d argue that all software, even government funded software should still be licensed under a traditional open source license, to resolve any legal ambiguity when used abroad under the terms of various international copyright treaties and agreements</p>\n<a href='#fnref:1' rev='footnote'>&#8617;</a></li><li id='fn:2'>\n<p>Although citizen-contributions to that project would theoretically not be public domain, thus necessitating a license, which should be clarified in the project&#8217;s documentation at the time of release to avoid potential issues with 21 U.S.C. § 1342.</p>\n<a href='#fnref:2' rev='footnote'>&#8617;</a></li><li id='fn:3'>\n<p>Although again, technically speaking the project as a whole would be licensed under GPL, individual code not dependent on the parent project could be used as a US Government Work.</p>\n<a href='#fnref:3' rev='footnote'>&#8617;</a></li><li id='fn:4'>\n<p>Unless you&#8217;re looking at the <a href='https://github.com/languages/ColdFusion'>vibrant open source cold fusion community</a>.</p>\n<a href='#fnref:4' rev='footnote'>&#8617;</a></li><li id='fn:5'>\n<p>FAR 52.227-14(c)(1)(i). Even if the contracting officer grants such rights, they do not take effect unless the contractor includes a copyright notice at the time of delivery, acknowledging the government&#8217;s sponsorship and indicating the contract number under which it was procured. See FAR 27.404(a)(5).</p>\n<a href='#fnref:5' rev='footnote'>&#8617;</a></li><li id='fn:6'>\n<p>General counsels across government already have enough ammunition to stymy progress.</p>\n<a href='#fnref:6' rev='footnote'>&#8617;</a></li><li id='fn:7'>\n<p>Often the last and least seen step in the enterprise development process.</p>\n<a href='#fnref:7' rev='footnote'>&#8617;</a></li><li id='fn:8'>\n<p>There&#8217;s been <a href='http://ben.balter.com/2012/04/15/cfpb-accepts-first-citizen-submitted-pull-request-on-behalf-of-federal-government/'>exactly one pull request to date</a> across all government github repos.</p>\n<a href='#fnref:8' rev='footnote'>&#8617;</a></li></ol></div>","related_posts":[{"url":"/2011/11/29/towards-a-more-agile-government/","title":"Towards a More Agile Government"},{"url":"/2012/04/10/whats-missing-from-cfpbs-awesome-new-source-code-policy/","title":"What's Missing from CFPB's Awesome New Source Code Policy"},{"url":"/2011/11/01/federal-agility-a-cultural-solution-to-a-technical-problem/","title":"Federal Agility: a Cultural Solution to a Technical Problem"},{"url":"/2012/03/05/wordpress-for-government-and-enterprise/","title":"WordPress for Government - A Problem of Perception"},{"url":"/2011/09/01/why-wordpress/","title":"Why WordPress"},{"url":"/2010/11/15/will-federal-contracting-officers-soon-have-their-heads-in-the-clouds/","title":"Will Federal Contracting Officers Soon Have Their Heads in the Clouds?\n"},{"url":"/2012/12/16/deprecate-management/","title":"Deprecate Management"},{"url":"/2011/04/27/fair-use-excerpting-and-copying-content-in-the-internet-ecosystem/","title":"Fair Use, Excerpting, and Copying Content in the Internet Ecosystem\n"},{"url":"/2012/10/15/open-source-is-not-a-verb/","title":"Open Source is not a verb"},{"url":"/2012/06/26/why-you-should-always-write-software-as-open-source/","title":"Why You Should Always Write Software as Open Source, Even When It's Never Going to Be"}]}